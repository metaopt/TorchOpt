Model-Agnostic Meta-Learning
============================

Meta reinforcement learning has achieved significant successes in various applications.
**Model-Agnostic Meta-Learning** (MAML) :cite:`MAML` is the pioneer one.
In this tutorial, we will show how to train MAML on few-shot Omniglot classification with TorchOpt step by step.
The full script is at `examples/MAML-RL/maml.py <https://github.com/metaopt/TorchOpt/blob/main/examples/few-shot/maml_omniglot.py>`_.

Contrary to existing differentiable optimizer libraries such as `higher <https://github.com/facebookresearch/higher>`_, which follows the PyTorch designing which leads to inflexible API, TorchOpt provides an easy way of construction through the code-level.


Overview
--------

In reinforcement learning, the agent interacts with environments to improve itself.


There are three types of data flow in RL training pipeline:

1. Load Dataset: ``action`` will be generated by agent and sent to environment;
2. Environment to agent: ``env.step`` takes action, and returns a tuple of ``(observation, reward, done, info)``;
3. Agent-environment interaction to agent training: the data generated by interaction will be stored and sent to the learner of agent.

In the following sections, we will set up (vectorized) environments, policy (with neural network), collector (with buffer), and trainer to successfully run the RL training and evaluation pipeline.
Here is the overall procedure:


Load Dataset
------------

First of all, you have to make an environment for your agent to interact with. You can use ``gym.make(environment_name)`` to make an environment for your agent. For environment interfaces, we follow the convention of `OpenAI Gym <https://github.com/openai/gym>`_. In your Python code, simply import torchopt and make the environment:
::

    from .support.omniglot_loaders import OmniglotNShot
    import torch

    device = torch.device('cuda:0')
    db = OmniglotNShot(
        '/tmp/omniglot-data',
        batchsz=args.task_num,
        n_way=args.n_way,
        k_shot=args.k_spt,
        k_query=args.k_qry,
        imgsz=28,
        rng=rng,
        device=device,
    )

CartPole-v0 includes a cart carrying a pole moving on a track. This is a simple environment with a discrete action space, for which DQN applies. You have to identify whether the action space is continuous or discrete and apply eligible algorithms. MAML :cite:`MAML`, for example, could only be applied to continuous action spaces, while almost all other policy gradient methods could be applied to both.

Here is the detail of useful fields of CartPole-v0:

- ``state``: the position of the cart, the velocity of the cart, the angle of the pole and the velocity of the tip of the pole;
- ``action``: can only be one of ``[0, 1, 2]``, for moving the cart left, no move, and right;
- ``reward``: each timestep you last, you will receive a +1 ``reward``;
- ``done``: if CartPole is out-of-range or timeout (the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center, or you last over 200 timesteps);
- ``info``: extra info from environment simulation.

The goal is to train a good policy that can get the highest reward in this environment.

Test
----

torchopt supports any user-defined PyTorch networks and optimizers. Yet, of course, the inputs and outputs must comply with torchopt's API. Here is an example:
::

    import torch, numpy as np
    from torch import nn

    class Net(nn.Module):
        def __init__(self, state_shape, action_shape):
            super().__init__()
            self.model = nn.Sequential(
                nn.Linear(np.prod(state_shape), 128), nn.ReLU(inplace=True),
                nn.Linear(128, 128), nn.ReLU(inplace=True),
                nn.Linear(128, 128), nn.ReLU(inplace=True),
                nn.Linear(128, np.prod(action_shape)),
            )

        def forward(self, obs, state=None, info={}):
            if not isinstance(obs, torch.Tensor):
                obs = torch.tensor(obs, dtype=torch.float)
            batch = obs.shape[0]
            logits = self.model(obs.view(batch, -1))
            return logits, state

    state_shape = env.observation_space.shape or env.observation_space.n
    action_shape = env.action_space.shape or env.action_space.n
    net = Net(state_shape, action_shape)
    optim = torch.optim.Adam(net.parameters(), lr=1e-3)


Train
-----

If you want to use the original ``gym.Env``:
::

    def train(db, net, meta_opt, epoch, log):
        net.train()
        n_train_iter = db.x_train.shape[0] // db.batchsz
        inner_opt = torchopt.MetaSGD(net, lr=1e-1)

        for batch_idx in range(n_train_iter):
            start_time = time.time()
            # Sample a batch of support and query images and labels.
            x_spt, y_spt, x_qry, y_qry = db.next()

            task_num, setsz, c_, h, w = x_spt.size()
            querysz = x_qry.size(1)

        # TODO: Maybe pull this out into a separate module so it
        # doesn't have to be duplicated between `train` and `test`?

        # Initialize the inner optimizer to adapt the parameters to
        # the support set.
            n_inner_iter = 5

            qry_losses = []
            qry_accs = []
            meta_opt.zero_grad()

            net_state_dict = torchopt.extract_state_dict(net)
            optim_state_dict = torchopt.extract_state_dict(inner_opt)
            for i in range(task_num):
            # Optimize the likelihood of the support set by taking
            # gradient steps w.r.t. the model's parameters.
            # This adapts the model's meta-parameters to the task.
            # higher is able to automatically keep copies of
            # your network's parameters as they are being updated.
                for _ in range(n_inner_iter):
                    spt_logits = net(x_spt[i])
                    spt_loss = F.cross_entropy(spt_logits, y_spt[i])
                    inner_opt.step(spt_loss)

            # The final set of adapted parameters will induce some
            # final loss and accuracy on the query dataset.
            # These will be used to update the model's meta-parameters.
                qry_logits = net(x_qry[i])
                qry_loss = F.cross_entropy(qry_logits, y_qry[i])
            qry_losses.append(qry_loss.detach())
                qry_acc = (qry_logits.argmax(dim=1) == y_qry[i]).sum().item() / querysz
                qry_accs.append(qry_acc)

            # Update the model's meta-parameters to optimize the query
            # losses across all of the tasks sampled in this batch.
            # This unrolls through the gradient steps.
                qry_loss.backward()

                torchopt.recover_state_dict(net, net_state_dict)
                torchopt.recover_state_dict(inner_opt, optim_state_dict)

            meta_opt.step()
            qry_losses = sum(qry_losses) / task_num
            qry_accs = 100. * sum(qry_accs) / task_num
            i = epoch + float(batch_idx) / n_train_iter
            iter_time = time.time() - start_time

            print(
                f'[Epoch {i:.2f}] Train Loss: {qry_losses:.2f} | Acc: {qry_accs:.2f} | Time: {iter_time:.2f}'
            )

            log.append(
                {
                    'epoch': i,
                    'loss': qry_losses,
                    'acc': qry_accs,
                    'mode': 'train',
                    'time': time.time(),
                }
            )

Plot
----

torchopt supports any user-defined PyTorch networks and optimizers. Yet, of course, the inputs and outputs must comply with torchopt's API. Here is an example:

::

    # Generally you should pull your plotting code out of your training
    # script but we are doing it here for brevity.
    df = pd.DataFrame(log)

    fig, ax = plt.subplots(figsize=(6, 4))
    train_df = df[df['mode'] == 'train']
    test_df = df[df['mode'] == 'test']
    ax.plot(train_df['epoch'], train_df['acc'], label='Train')
    ax.plot(test_df['epoch'], test_df['acc'], label='Test')
    ax.set_xlabel('Epoch')
    ax.set_ylabel('Accuracy')
    ax.set_ylim(70, 100)
    fig.legend(ncol=2, loc='lower right')
    fig.tight_layout()
    fname = 'maml-accs.png'
    print(f'--- Plotting accuracy to {fname}')
    fig.savefig(fname)
    plt.close(fig)

.. .. image:: /_static/images/maml-accs.png
..     :align: center
..     :height: 300


.. rubric:: References

.. bibliography:: /refs.bib
    :style: unsrtalpha

