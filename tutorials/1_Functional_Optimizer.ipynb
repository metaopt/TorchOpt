{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TorchOpt as Functional Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[<img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\">](https://colab.research.google.com/drive/1h005zH00arR5IgeSUjNETnP_r4A_-oMr?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we will introduce how TorchOpt can be treated as functional optimizer to conduct normal optimization with functional programing style. We will also illustrate how to conduct differentiable optimization with functional programing in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first part, we will illustrate how TorchOpt can be used as a functional optimizer. We compare it with different api in Jax and PyTorch to help understand the similarity and dissimilarity. We use simple network, adam optimizer and  MSE loss objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import functorch\n",
    "import torch.autograd\n",
    "import torch.nn as nn\n",
    "import optax\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "\n",
    "import torchopt\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(dim, 1, bias=False)\n",
    "        self.fc.weight.data = torch.ones_like(self.fc.weight.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Original JAX implementation\n",
    "\n",
    "The first example is jax implementation coupled with optax, which belongs to functional programing style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def origin_jax():\n",
    "    learning_rate = 1.\n",
    "    batch_size = 1\n",
    "    dim = 1\n",
    "    optimizer = optax.adam(learning_rate)\n",
    "    # Obtain the `opt_state` that contains statistics for the optimizer.\n",
    "    params = {'w': jnp.ones((dim, 1))}\n",
    "    opt_state = optimizer.init(params)\n",
    "\n",
    "    def compute_loss(params, x, y): return (\n",
    "        (jnp.matmul(x, params['w']) - y) ** 2).sum()\n",
    "\n",
    "    xs = 2 * jnp.ones((batch_size, dim))\n",
    "    ys = jnp.ones((batch_size, ))\n",
    "    grads = jax.grad(compute_loss)(params, xs, ys)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    print(params)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'w': DeviceArray([[1.]], dtype=float32)}\n",
      "{'w': DeviceArray([[6.67572e-06]], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "origin_jax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Functorch with TorchOpt\n",
    "\n",
    "The Second example is functorch coupled with TorchOpt. It basically follows the same structure with the jax example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interact_with_functorch():\n",
    "    batch_size = 1\n",
    "    dim = 1\n",
    "    net = Net(dim)\n",
    "    func, params = functorch.make_functional(net)\n",
    "\n",
    "    lr = 1.\n",
    "    optimizer = torchopt.adam(lr)\n",
    "\n",
    "    opt_state = optimizer.init(params)\n",
    "\n",
    "    xs = 2 * torch.ones(batch_size, dim)\n",
    "    ys = torch.ones(batch_size)\n",
    "\n",
    "    pred = func(params, xs)\n",
    "    loss = ((pred - ys) ** 2).sum()\n",
    "    grad = torch.autograd.grad(loss, params)\n",
    "    updates, opt_state = optimizer.update(grad, opt_state)\n",
    "    print(params)\n",
    "    params = torchopt.apply_updates(params, updates)\n",
    "    print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Parameter containing:\n",
      "tensor([[1.]], requires_grad=True),)\n",
      "(Parameter containing:\n",
      "tensor([[0.]], requires_grad=True),)\n"
     ]
    }
   ],
   "source": [
    "interact_with_functorch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Full TorchOpt\n",
    "\n",
    "The Third example is to illustrate that TorchOpt can also directly replace torch.optim with exactly the same usage. Note the API \n",
    "difference happens between torchopt.adam() and torchopt.Adam(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_torchopt():\n",
    "    batch_size = 1\n",
    "    dim = 1\n",
    "    net = Net(dim)\n",
    "\n",
    "    lr = 1.\n",
    "    optim = torchopt.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    xs = 2 * torch.ones(batch_size, dim)\n",
    "    ys = torch.ones(batch_size)\n",
    "\n",
    "    pred = net(xs)\n",
    "    loss = ((pred - ys) ** 2).sum()\n",
    "\n",
    "    print(net.fc.weight)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    print(net.fc.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[1.]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "full_torchopt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Original PyTorch\n",
    "\n",
    "The final example is to original PyTorch example with torch.optim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def origin_torch():\n",
    "    batch_size = 1\n",
    "    dim = 1\n",
    "    net = Net(dim)\n",
    "\n",
    "    lr = 1.\n",
    "    optim = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    xs = 2 * torch.ones(batch_size, dim)\n",
    "    ys = torch.ones(batch_size)\n",
    "\n",
    "    pred = net(xs)\n",
    "    loss = ((pred - ys) ** 2).sum()\n",
    "\n",
    "    print(net.fc.weight)\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "    print(net.fc.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[1.]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[1.1921e-07]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "origin_torch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Differentiable Optimization with functional optimizor\n",
    "Coupled with functional optimizer, you can conduct differentiable optimization by setting the inplce flag as False in update and apply_updates function. (which might be helpful for meta-learning algorithm implementation with functional programing style). \n",
    "\n",
    "Note that torchopt.SGD, torchopt.Adam do not support differentiable optimization. Refer to the Meta Optimizer notebook for pytorch-like differentiable optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def differentiable():\n",
    "    batch_size = 1\n",
    "    dim = 1\n",
    "    net = Net(dim)\n",
    "    func, params = functorch.make_functional(net)\n",
    "\n",
    "    lr = 1.\n",
    "    # sgd example\n",
    "    optimizer = torchopt.sgd(lr)\n",
    "    meta_param = torch.tensor(1., requires_grad=True)\n",
    "\n",
    "    opt_state = optimizer.init(params)\n",
    "\n",
    "    xs = torch.ones(batch_size, dim)\n",
    "    ys = torch.ones(batch_size)\n",
    "\n",
    "    pred = func(params, xs)\n",
    "    # where meta_param is used\n",
    "    pred = pred + meta_param\n",
    "    loss = ((pred - ys) ** 2).sum()\n",
    "    grad = torch.autograd.grad(loss, params, create_graph=True)\n",
    "    updates, opt_state = optimizer.update(grad, opt_state, inplace=False)\n",
    "    params = torchopt.apply_updates(params, updates, inplace=False)\n",
    "\n",
    "    pred = func(params, xs)\n",
    "    loss = ((pred - ys) ** 2).sum()\n",
    "    loss.backward()\n",
    "\n",
    "    print(meta_param.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.)\n"
     ]
    }
   ],
   "source": [
    "differentiable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Track the gradient of moment\n",
    "Note that most modern optimizers involve moment term in the gradient update (basically only SGD with momentum = 0 does not involve). We provide an option for user to choose whether to also  track the meta-gradient through moment term. The default option is `moment_requires_grad=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torchopt.adam(lr=1., moment_requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torchopt.adam(lr=1., moment_requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torchopt.sgd(lr=1., momentum=0.8, moment_requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Accletated Optimizer\n",
    "Users can use acclerated optimizer by seeting the `use_accelerated_op` as True. Currently we only support the Adam optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check whether the accelerated_op is avariable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchopt.accelerated_op_available(torch.device(\"cpu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchopt.accelerated_op_available(torch.device(\"cuda\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(1).cuda()\n",
    "optim = torchopt.Adam(net.parameters(), lr=1., use_accelerated_op=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torchopt.adam(lr=1., use_accelerated_op=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "238ad0feaa04228775e5e27229169b0e3e76c0e018d5a6d65c4906ccad5c5a9e"
  },
  "kernelspec": {
   "display_name": "TorchOpt",
   "language": "python",
   "name": "optorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
